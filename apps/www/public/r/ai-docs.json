{
  "$schema": "https://ui.shadcn.com/schema/registry-item.json",
  "name": "ai-docs",
  "type": "registry:file",
  "title": "AI",
  "description": "AI-powered writing assistance.",
  "files": [
    {
      "path": "../../docs/(plugins)/(ai)/ai.mdx",
      "content": "---\ntitle: AI\ndescription: AI-powered writing assistance.\ndocs:\n  - route: https://pro.platejs.org/docs/examples/ai\n    title: Plus\n---\n\n<ComponentPreview name=\"ai-demo\" />\n\n<PackageInfo>\n\n## Features\n\n- **Intelligent Command Menu**: Combobox interface with predefined AI commands for generation and editing\n- **Multiple Trigger Modes**:\n  - **Cursor Mode**: Trigger at block end with space\n  - **Selection Mode**: Trigger with selected text\n  - **Block Selection Mode**: Trigger with selected blocks\n- **Response Modes**:\n  - **Chat Mode**: Preview responses with accept/reject options\n  - **Insert Mode**: Direct content insertion with markdown streaming\n- **Smart Content Processing**: Optimized chunking for tables, code blocks, and complex structures\n- **Streaming Responses**: Real-time AI content generation with support for:\n  - **Table Streaming**: Seamless streaming into table cells\n  - **Column Streaming**: Direct streaming into column layouts\n  - **MDX Tag Handling**: Proper preservation of custom MDX elements during streaming\n- **Markdown Integration**: Full support for Markdown syntax in AI responses\n- **Customizable Prompts**: Template system for user and system prompts\n- **Built-in Vercel AI SDK Support**: Ready-to-use chat API integration\n\n</PackageInfo>\n\n## Kit Usage\n\n<Steps>\n\n### Installation\n\nThe fastest way to add AI functionality is with the `AIKit`, which includes pre-configured `AIPlugin` and `AIChatPlugin` along with cursor overlay and markdown support and their [Plate UI](/docs/installation/plate-ui) components.\n\n<ComponentSource name=\"ai-kit\" />\n\n- [`AIMenu`](/docs/components/ai-menu): Renders the AI command interface\n- [`AILoadingBar`](/docs/components/ai-loading-bar): Shows AI processing status\n- [`AIAnchorElement`](/docs/components/ai-anchor-element): Anchor element for the AI Menu\n- [`AILeaf`](/docs/components/ai-leaf): Renders AI-generated content with visual distinction\n\n### Add Kit\n\n```tsx\nimport { createPlateEditor } from 'platejs/react';\nimport { AIKit } from '@/components/editor/plugins/ai-kit';\n\nconst editor = createPlateEditor({\n  plugins: [\n    // ...otherPlugins,\n    ...AIKit,\n  ],\n});\n```\n\n### Add API Route\n\nAI functionality requires a server-side API endpoint. Add the pre-configured AI command route:\n\n<ComponentSource name=\"ai-api\" />\n\n### Configure Environment\n\nEnsure your OpenAI API key is set in your environment variables:\n\n```bash title=\".env.local\"\nOPENAI_API_KEY=\"your-api-key\"\n```\n\n</Steps>\n\n## Manual Usage\n\n<Steps>\n\n### Installation\n\n```bash\nnpm install @platejs/ai @platejs/selection @platejs/markdown @platejs/basic-nodes\n```\n\n### Add Plugins\n\n```tsx\nimport { AIPlugin, AIChatPlugin } from '@platejs/ai/react';\nimport { createPlateEditor } from 'platejs/react';\nimport { MarkdownKit } from '@/components/editor/plugins/markdown-kit';\n\nconst editor = createPlateEditor({\n  plugins: [\n    // ...otherPlugins,\n    ...MarkdownKit, // Required for AI content processing\n    AIPlugin,\n    AIChatPlugin,\n  ],\n});\n```\n\n- [`MarkdownKit`](/docs/markdown): Required for processing AI responses with Markdown syntax and MDX support.\n- `AIPlugin`: Core plugin for AI content management and transforms.\n- `AIChatPlugin`: Handles AI chat interface, streaming, and user interactions.\n\n### Configure Plugins\n\nCreate the extended `aiChatPlugin` with basic configuration:\n\n```tsx\nimport type { AIChatPluginConfig } from '@platejs/ai/react';\nimport type { UseChatOptions } from 'ai/react';\n\nimport { KEYS, PathApi } from 'platejs';\nimport { streamInsertChunk, withAIBatch } from '@platejs/ai';\nimport { AIChatPlugin, AIPlugin, useChatChunk } from '@platejs/ai/react';\nimport { usePluginOption } from 'platejs/react';\nimport { MarkdownKit } from '@/components/editor/plugins/markdown-kit';\nimport { AILoadingBar, AIMenu } from '@/components/ui/ai-menu';\nimport { AIAnchorElement, AILeaf } from '@/components/ui/ai-node';\n\nexport const aiChatPlugin = AIChatPlugin.extend({\n  options: {\n    chatOptions: {\n      api: '/api/ai/command',\n      body: {},\n    } as UseChatOptions,\n  },\n  render: {\n    afterContainer: AILoadingBar,\n    afterEditable: AIMenu,\n    node: AIAnchorElement,\n  },\n  shortcuts: { show: { keys: 'mod+j' } },\n});\n\nconst plugins = [\n  // ...otherPlugins,\n  ...MarkdownKit,\n  AIPlugin.withComponent(AILeaf),\n  aiChatPlugin,\n];\n```\n\n- `chatOptions`: Configuration for the Vercel AI SDK `useChat` hook.\n- `render`: UI components for the AI interface.\n- `shortcuts`: Keyboard shortcuts (`Cmd+J` to show AI menu).\n\n### Add Streaming with useHooks\n\nThe `useChatChunk` hook processes streaming AI responses in real-time, handling content insertion and chunk management. It monitors the chat state and processes incoming text chunks, inserting them into the editor as they arrive:\n\n```tsx\nexport const aiChatPlugin = AIChatPlugin.extend({\n  // ... previous options\n  useHooks: ({ editor, getOption }) => {\n    const mode = usePluginOption(\n      { key: KEYS.aiChat } as AIChatPluginConfig,\n      'mode'\n    );\n\n    useChatChunk({\n      onChunk: ({ chunk, isFirst, nodes }) => {\n        if (isFirst && mode == 'insert') {\n          editor.tf.withoutSaving(() => {\n            editor.tf.insertNodes(\n              {\n                children: [{ text: '' }],\n                type: KEYS.aiChat,\n              },\n              {\n                at: PathApi.next(editor.selection!.focus.path.slice(0, 1)),\n              }\n            );\n          });\n          editor.setOption(AIChatPlugin, 'streaming', true);\n        }\n\n        if (mode === 'insert' && nodes.length > 0) {\n          withAIBatch(\n            editor,\n            () => {\n              if (!getOption('streaming')) return;\n              editor.tf.withScrolling(() => {\n                streamInsertChunk(editor, chunk, {\n                  textProps: {\n                    ai: true,\n                  },\n                });\n              });\n            },\n            { split: isFirst }\n          );\n        }\n      },\n      onFinish: () => {\n        editor.setOption(AIChatPlugin, 'streaming', false);\n        editor.setOption(AIChatPlugin, '_blockChunks', '');\n        editor.setOption(AIChatPlugin, '_blockPath', null);\n      },\n    });\n  },\n});\n```\n\n- `onChunk`: Handles each streaming chunk, creating AI nodes on first chunk and inserting content in real-time\n- `onFinish`: Cleans up streaming state when the response completes\n- Uses `withAIBatch` and `streamInsertChunk` for optimized content insertion\n\n### System Prompt\n\nThe system prompt defines the AI's role and behavior. You can customize the `systemTemplate` in your extended plugin:\n\n```tsx\nexport const customAIChatPlugin = AIChatPlugin.extend({\n  options: {\n    systemTemplate: ({ isBlockSelecting, isSelecting }) => {\n      const customSystem = `You are a technical documentation assistant specialized in code and API documentation.\n\nRules:\n- Provide accurate, well-structured technical content\n- Use appropriate code formatting and syntax highlighting\n- Include relevant examples and best practices\n- Maintain consistent documentation style\n- CRITICAL: DO NOT remove or modify custom MDX tags unless explicitly requested.\n- CRITICAL: Distinguish between INSTRUCTIONS and QUESTIONS.`;\n\n      return isBlockSelecting\n        ? `${customSystem}\n- <Selection> represents the full blocks of text the user has selected and wants to modify or ask about.\n- Your response should be a direct replacement for the entire <Selection>.\n- Maintain the overall structure and formatting of the selected blocks, unless explicitly instructed otherwise.\n<Selection>\n{block}\n</Selection>`\n        : isSelecting\n          ? `${customSystem}\n- <Block> is the block of text containing the user's selection, providing context.\n- <Selection> is the specific text the user has selected in the block and wants to modify or ask about.\n- Consider the context provided by <Block>, but only modify <Selection>.\n<Block>\n{block}\n</Block>\n<Selection>\n{selection}\n</Selection>`\n          : `${customSystem}\n- <Block> is the current block of text the user is working on.\n\n<Block>\n{block}\n</Block>`;\n    },\n    // ...other options\n  },\n}),\n```\n\n### User Prompt\n\nCustomize how user prompts are formatted and contextualized in your extended plugin:\n\n```tsx\nexport const customAIChatPlugin = AIChatPlugin.extend({\n  options: {\n    promptTemplate: ({ isBlockSelecting, isSelecting }) => {\n      return isBlockSelecting\n        ? `<Reminder>\nIf this is a question, provide a helpful and concise answer about <Selection>.\nIf this is an instruction, provide ONLY the content to replace the entire <Selection>. No explanations.\nAnalyze and improve the following content blocks maintaining structure and clarity.\nNEVER write <Block> or <Selection>.\n</Reminder>\n{prompt} about <Selection>`\n        : isSelecting\n          ? `<Reminder>\nIf this is a question, provide a helpful and concise answer about <Selection>.\nIf this is an instruction, provide ONLY the text to replace <Selection>. No explanations.\nEnsure it fits seamlessly within <Block>. If <Block> is empty, write ONE random sentence.\nNEVER write <Block> or <Selection>.\n</Reminder>\n{prompt} about <Selection>`\n          : `<Reminder>\nCRITICAL: NEVER write <Block>.\nContinue or improve the content naturally.\n</Reminder>\n{prompt}`;\n    },\n    // ...other options\n  },\n}),\n```\n\n### Add API Route\n\nCreate an API route handler with optimized streaming for different content types:\n\n```tsx title=\"app/api/ai/command/route.ts\"\nimport type { NextRequest } from 'next/server';\n\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { convertToCoreMessages, streamText } from 'ai';\nimport { NextResponse } from 'next/server';\n\nimport { markdownJoinerTransform } from '@/registry/lib/markdown-joiner-transform';\n\nexport async function POST(req: NextRequest) {\n  const { apiKey: key, messages, system } = await req.json();\n\n  const apiKey = key || process.env.OPENAI_API_KEY;\n\n  if (!apiKey) {\n    return NextResponse.json(\n      { error: 'Missing OpenAI API key.' },\n      { status: 401 }\n    );\n  }\n\n  const openai = createOpenAI({ apiKey });\n\n  try {\n    const result = streamText({\n      experimental_transform: markdownJoinerTransform(),\n      maxTokens: 2048,\n      messages: convertToCoreMessages(messages),\n      model: openai('gpt-4o'),\n      system: system,\n    });\n\n    return result.toDataStreamResponse();\n  } catch {\n    return NextResponse.json(\n      { error: 'Failed to process AI request' },\n      { status: 500 }\n    );\n  }\n}\n```\n\nThen, set your `OPENAI_API_KEY` in `.env.local`.\n\n### Add Toolbar Button\n\nYou can add [`AIToolbarButton`](/docs/components/ai-toolbar-button) to your [Toolbar](/docs/toolbar) to open the AI menu.\n\n</Steps>\n\n## Keyboard Shortcuts\n\n<KeyTable>\n  <KeyTableItem hotkey=\"Space\">\n    Open AI menu in empty block (cursor mode)\n  </KeyTableItem>\n  <KeyTableItem hotkey=\"Cmd + J\">\n    Open AI menu (cursor or selection mode)\n  </KeyTableItem>\n  <KeyTableItem hotkey=\"Escape\">Close AI menu</KeyTableItem>\n</KeyTable>\n\n## Streaming Example\n\n<ComponentPreview name=\"markdown-streaming-demo\" />\n\n\n## Plate Plus\n\n\n\n## Customization\n\n### Adding Custom AI Commands\n\n<ComponentSource name=\"ai-menu\" />\n\nYou can extend the AI menu with custom commands by adding new items to the `aiChatItems` object and updating the menu state items.\n\n#### Simple Custom Command\n\nAdd a basic command that submits a custom prompt:\n\n```tsx\n// Add to your ai-menu.tsx aiChatItems object\nsummarizeInBullets: {\n  icon: <ListIcon />,\n  label: 'Summarize in bullets',\n  value: 'summarizeInBullets',\n  onSelect: ({ editor }) => {\n    void editor.getApi(AIChatPlugin).aiChat.submit({\n      prompt: 'Summarize this content as bullet points',\n    });\n  },\n},\n```\n\n#### Command with Complex Logic\n\nCreate commands with client-side logic before submission:\n\n```tsx\ngenerateTOC: {\n  icon: <BookIcon />,\n  label: 'Generate table of contents',\n  value: 'generateTOC',\n  onSelect: ({ editor }) => {\n    // Check if document has headings\n    const headings = editor.api.nodes({\n      match: (n) => ['h1', 'h2', 'h3'].includes(n.type as string),\n    });\n\n    if (headings.length === 0) {\n      void editor.getApi(AIChatPlugin).aiChat.submit({\n        mode: 'insert',\n        prompt: 'Create a table of contents with sample headings for this document',\n      });\n    } else {\n      void editor.getApi(AIChatPlugin).aiChat.submit({\n        mode: 'insert',\n        prompt: 'Generate a table of contents based on the existing headings',\n      });\n    }\n  },\n},\n```\n\n#### Understanding Menu States\n\nThe AI menu adapts to different contexts based on user selection and AI response state:\n\n```tsx\nconst menuState = React.useMemo(() => {\n  // If AI has already responded, show suggestion actions\n  if (messages && messages.length > 0) {\n    return isSelecting ? 'selectionSuggestion' : 'cursorSuggestion';\n  }\n  \n  // If no AI response yet, show command actions\n  return isSelecting ? 'selectionCommand' : 'cursorCommand';\n}, [isSelecting, messages]);\n```\n\n**Menu States:**\n- `cursorCommand`: No selection, no AI response → Show generation commands (Continue writing, Summarize, etc.)\n- `selectionCommand`: Text selected, no AI response → Show editing commands (Improve writing, Fix spelling, etc.)  \n- `cursorSuggestion`: No selection, AI responded → Show suggestion actions (Accept, Discard, Try again)\n- `selectionSuggestion`: Text selected, AI responded → Show replacement actions (Replace selection, Insert below, etc.)\n\n#### Update Menu States\n\nAdd your custom commands to the appropriate menu states in `menuStateItems`:\n\n```tsx\nconst menuStateItems: Record<EditorChatState, { items: any[] }[]> = {\n  cursorCommand: [\n    {\n      items: [\n        aiChatItems.generateTOC,\n        aiChatItems.summarizeInBullets,\n        // ... existing items\n      ],\n    },\n  ],\n  selectionCommand: [\n    {\n      items: [\n        aiChatItems.summarizeInBullets, // Works for selected text too\n        // ... existing items\n      ],\n    },\n  ],\n  // ... other states\n};\n```\n\n### Switching AI Models\n\nConfigure different AI models and providers in your API route:\n\n```tsx title=\"app/api/ai/command/route.ts\"\nimport { createOpenAI } from '@ai-sdk/openai';\nimport { createAnthropic } from '@ai-sdk/anthropic';\n\nexport async function POST(req: NextRequest) {\n  const { model = 'gpt-4o', provider = 'openai', ...rest } = await req.json();\n\n  let aiProvider;\n  \n  switch (provider) {\n    case 'anthropic':\n      aiProvider = createAnthropic({ apiKey: process.env.ANTHROPIC_API_KEY });\n      break;\n    case 'openai':\n    default:\n      aiProvider = createOpenAI({ apiKey: process.env.OPENAI_API_KEY });\n      break;\n  }\n\n  const result = streamText({\n    model: aiProvider(model),\n    // ... other options\n  });\n\n  return result.toDataStreamResponse();\n}\n```\n\nConfigure the model in your `aiChatPlugin`:\n\n```tsx\nexport const aiChatPlugin = AIChatPlugin.extend({\n  options: {\n    chatOptions: {\n      api: '/api/ai/command',\n      body: {\n        model: 'gpt-4o-mini', // or 'claude-4-sonnet'\n        provider: 'openai',   // or 'anthropic'\n      },\n    },\n    // ... other options\n  },\n});\n```\n\nFor more AI providers and models, see the [Vercel AI SDK documentation](https://sdk.vercel.ai/providers/ai-sdk-providers).\n\n### Custom Streaming Optimization\n\nOptimize streaming performance for specific content types with custom chunking strategies:\n\n```tsx title=\"app/api/ai/command/route.ts\"\nconst customChunking = (buffer: string) => {\n  // Detect JSON content for slower chunking\n  if (buffer.includes('{') && buffer.includes('}')) {\n    const jsonMatch = /\\{[^}]*\\}/g.exec(buffer);\n    if (jsonMatch) {\n      return buffer.slice(0, jsonMatch.index + jsonMatch[0].length);\n    }\n  }\n\n  // Detect code blocks for line-based chunking\n  if (buffer.includes('```')) {\n    const lineMatch = /\\n+/m.exec(buffer);\n    return lineMatch ? buffer.slice(0, lineMatch.index + lineMatch[0].length) : null;\n  }\n\n  // Default word chunking\n  const wordMatch = /\\S+\\s+/m.exec(buffer);\n  return wordMatch ? buffer.slice(0, wordMatch.index + wordMatch[0].length) : null;\n};\n\n// Use in your streamText configuration\nconst result = streamText({\n  experimental_transform: smoothStream({\n    chunking: customChunking,\n    delayInMs: (buffer) => {\n      // Slower for complex content, faster for simple text\n      return buffer.includes('```') || buffer.includes('{') ? 80 : 20;\n    },\n  }),\n  // ... other options\n});\n```\n\n### Security Considerations\n\nImplement security best practices for AI functionality:\n\n```tsx title=\"app/api/ai/command/route.ts\"\nexport async function POST(req: NextRequest) {\n  const { messages, system } = await req.json();\n\n  // Validate request structure\n  if (!messages || !Array.isArray(messages)) {\n    return NextResponse.json({ error: 'Invalid messages' }, { status: 400 });\n  }\n\n  // Content length validation\n  const totalContent = messages.map(m => m.content).join('');\n  if (totalContent.length > 50000) {\n    return NextResponse.json({ error: 'Content too long' }, { status: 413 });\n  }\n\n  // Rate limiting (implement with your preferred solution)\n  // await rateLimit(req);\n\n  // Content filtering (optional)\n  // const filteredMessages = await filterContent(messages);\n\n  // Process AI request...\n}\n```\n\n**Security Guidelines:**\n- **Validate Input**: Always validate and sanitize user prompts\n- **Rate Limiting**: Implement rate limiting on AI endpoints  \n- **Content Filtering**: Consider content filtering for responses\n- **API Key Security**: Never expose API keys client-side\n- **User Privacy**: Be mindful of data sent to AI models\n\n## Plugins\n\n### `AIPlugin`\n\nCore plugin that extends the editor with AI content management capabilities.\n\n<API name=\"AIPlugin\">\n<APIOptions>\n  <APIItem name=\"node\" type=\"object\">\n    Node configuration for AI leaf elements.\n    - `isLeaf: true`: AI content is treated as leaf nodes\n    - `isDecoration: false`: Not used for decorations\n  </APIItem>\n</APIOptions>\n</API>\n\n### `AIChatPlugin`\n\nMain plugin that enables AI chat operations, streaming, and user interface interactions.\n\n<API name=\"AIChatPlugin\">\n<APIOptions>\n  <APIItem name=\"chatOptions\" type=\"UseChatOptions\">\n    Configuration options for the Vercel AI SDK `useChat` hook.\n    - `api`: API endpoint for AI requests\n    - `body`: Additional request body parameters\n  </APIItem>\n  <APIItem name=\"mode\" type=\"'chat' | 'insert'\" optional>\n    Specifies how assistant messages are handled:\n    - `'chat'`: Shows preview with accept/reject options\n    - `'insert'`: Directly inserts content into editor\n    - **Default:** `'chat'`\n  </APIItem>\n  <APIItem name=\"open\" type=\"boolean\" optional>\n    Whether the AI chat interface is open.\n    - **Default:** `false`\n  </APIItem>\n  <APIItem name=\"streaming\" type=\"boolean\" optional>\n    Whether AI response is currently streaming.\n    - **Default:** `false`\n  </APIItem>\n  <APIItem name=\"promptTemplate\" type=\"(props: EditorPromptParams) => string\" optional>\n    Template for generating user prompts. Supports placeholders:\n    - `{block}`: Markdown of blocks in selection\n    - `{editor}`: Markdown of entire editor content\n    - `{selection}`: Markdown of current selection\n    - `{prompt}`: Actual user prompt\n    - **Default:** `'{prompt}'`\n  </APIItem>\n  <APIItem name=\"systemTemplate\" type=\"(props: EditorPromptParams) => string | void\" optional>\n    Template for system messages. Supports same placeholders as `promptTemplate`.\n  </APIItem>\n  <APIItem name=\"aiEditor\" type=\"SlateEditor | null\" optional>\n    The editor instance used to generate AI responses.\n    - **Default:** `null`\n  </APIItem>\n  <APIItem name=\"chat\" type=\"Partial<UseChatHelpers>\" optional>\n    Chat helpers returned by `useChat` hook.\n  </APIItem>\n</APIOptions>\n</API>\n\n## API\n\n### `api.aiChat.accept()`\n\nAccepts the current AI suggestion:\n- Removes AI marks from the content\n- Hides the AI chat interface\n- Focuses the editor\n\n### `api.aiChat.insertBelow()`\n\nInserts AI-generated content below the current block.\n\nHandles both block selection and normal selection modes:\n- In block selection: Inserts after the last selected block, applying formatting from the last block\n- In normal selection: Inserts after the current block, applying formatting from the current block\n\n<API name=\"insertBelow\">\n<APIParameters>\n  <APIItem name=\"sourceEditor\" type=\"PlateEditor\">\n    Editor containing the content to insert.\n  </APIItem>\n  <APIItem name=\"options\" type=\"object\" optional>\n    Options for insertion behavior.\n  </APIItem>\n</APIParameters>\n\n<APIOptions type=\"object\">\n  <APIItem name=\"format\" type=\"'all' | 'none' | 'single'\" optional>\n    Format to apply:\n    - `'all'`: Apply formatting to all blocks\n    - `'none'`: Insert without formatting\n    - `'single'`: Apply formatting only to first block\n    - **Default:** `'single'`\n  </APIItem>\n</APIOptions>\n</API>\n\n### `api.aiChat.replaceSelection()`\n\nReplaces the current selection with AI-generated content.\n\nHandles different selection modes:\n- Single block selection: Replaces the selected block, applying its formatting to inserted content based on format option\n- Multiple block selection: Replaces all selected blocks\n  - With `format: 'none'` or `'single'`: Preserves original formatting\n  - With `format: 'all'`: Applies first block's formatting to all content\n- Normal selection: Replaces the current selection while maintaining surrounding context\n\n<API name=\"replaceSelection\">\n<APIParameters>\n  <APIItem name=\"sourceEditor\" type=\"PlateEditor\">\n    Editor containing the replacement content.\n  </APIItem>\n  <APIItem name=\"options\" type=\"object\" optional>\n    Options for replacement behavior.\n  </APIItem>\n</APIParameters>\n\n<APIOptions type=\"object\">\n  <APIItem name=\"format\" type=\"'all' | 'none' | 'single'\" optional>\n    Format to apply:\n    - `'all'`: Apply formatting to all blocks\n    - `'none'`: Replace without formatting\n    - `'single'`: Apply formatting only to first block\n    - **Default:** `'single'`\n  </APIItem>\n</APIOptions>\n</API>\n\n### `api.aiChat.reset()`\n\nResets the chat state:\n- Stops any ongoing generation\n- Clears chat messages\n- Removes all AI nodes from the editor\n\n### `api.aiChat.node()`\n\nGets the AI chat node entry.\n\n<API name=\"node\">\n<APIParameters>\n  <APIItem name=\"options\" type=\"EditorNodesOptions & { anchor?: boolean; streaming?: boolean }\" optional>\n    Options for finding the node.\n  </APIItem>\n</APIParameters>\n\n<APIOptions type=\"EditorNodesOptions & { anchor?: boolean; streaming?: boolean }\">\n  <APIItem name=\"anchor\" type=\"boolean\" optional>\n    When true, finds nodes with type matching the plugin type.\n    - **Default:** `false`\n  </APIItem>\n  <APIItem name=\"streaming\" type=\"boolean\" optional>\n    When true, finds streaming AI nodes.\n    - **Default:** `false`\n  </APIItem>\n</APIOptions>\n\n<APIReturns type=\"NodeEntry | undefined\">\n  The found node entry or undefined if not found.\n</APIReturns>\n</API>\n\n### `api.aiChat.reload()`\n\nReloads the current AI chat:\n- In insert mode: Undoes previous AI changes\n- Reloads the chat with the current system prompt\n\n### `api.aiChat.show()`\n\nShows the AI chat interface:\n- Resets the chat state\n- Clears messages\n- Sets the open state to true\n\n### `api.aiChat.hide()`\n\nHides the AI chat interface:\n- Resets the chat state\n- Sets the open state to false\n- Focuses the editor\n- Removes the AI anchor\n\n### `api.aiChat.stop()`\n\nStops the current AI generation:\n- Sets streaming state to false\n- Calls the chat stop function\n\n### `api.aiChat.submit()`\n\nSubmits a prompt to generate AI content.\n\n<API name=\"submit\">\n<APIParameters>\n  <APIItem name=\"options\" type=\"SubmitOptions\" optional>\n    Options for the submission.\n  </APIItem>\n</APIParameters>\n\n<APIOptions type=\"SubmitOptions\">\n  <APIItem name=\"mode\" type=\"'chat' | 'insert'\" optional>\n    Mode to use. In insert mode, undoes previous AI changes before submitting.\n    - **Default:** `'chat'` for selection, `'insert'` otherwise\n  </APIItem>\n  <APIItem name=\"prompt\" type=\"string\" optional>\n    Custom prompt to submit.\n    - **Default:** Uses chat input if not provided\n  </APIItem>\n  <APIItem name=\"system\" type=\"string\" optional>\n    Custom system message for this request.\n  </APIItem>\n</APIOptions>\n</API>\n\n## Transforms\n\n### `tf.aiChat.removeAnchor()`\n\nRemoves the AI chat anchor node from the editor.\n\n<API name=\"removeAnchor\">\n<APIParameters>\n  <APIItem name=\"options\" type=\"EditorNodesOptions\" optional>\n    Options for finding nodes to remove.\n  </APIItem>\n</APIParameters>\n</API>\n\n### `tf.aiChat.accept()`\n\nAccepts the current AI suggestion and integrates it into the editor content.\n\n### `tf.aiChat.insertBelow()`\n\nTransform that inserts AI content below the current block.\n\n### `tf.aiChat.replaceSelection()`\n\nTransform that replaces the current selection with AI content.\n\n### `tf.ai.insertNodes()`\n\nInserts AI-generated nodes with the AI mark.\n\n<API name=\"tf.ai.insertNodes\">\n<APIParameters>\n  <APIItem name=\"nodes\" type=\"Descendant[]\">\n    Nodes to insert with AI mark.\n  </APIItem>\n  <APIItem name=\"options\" type=\"InsertNodesOptions\" optional>\n    Options for inserting nodes.\n  </APIItem>\n</APIParameters>\n\n<APIOptions type=\"InsertNodesOptions\">\n  <APIItem name=\"target\" type=\"Path\" optional>\n    Target path for insertion.\n    - **Default:** Current selection\n  </APIItem>\n</APIOptions>\n</API>\n\n### `tf.ai.removeMarks()`\n\nRemoves AI marks from nodes in the specified location.\n\n<API name=\"tf.ai.removeMarks\">\n<APIParameters>\n  <APIItem name=\"options\" type=\"RemoveMarksOptions\" optional>\n    Options for removing marks.\n  </APIItem>\n</APIParameters>\n\n<APIOptions type=\"RemoveMarksOptions\">\n  <APIItem name=\"at\" type=\"Location\" optional>\n    Location to remove marks from.\n    - **Default:** Entire document\n  </APIItem>\n</APIOptions>\n</API>\n\n### `tf.ai.removeNodes()`\n\nRemoves nodes that have the AI mark.\n\n<API name=\"tf.ai.removeNodes\">\n<APIParameters>\n  <APIItem name=\"options\" type=\"RemoveNodesOptions\" optional>\n    Options for removing nodes.\n  </APIItem>\n</APIParameters>\n\n<APIOptions type=\"RemoveNodesOptions\">\n  <APIItem name=\"at\" type=\"Path\" optional>\n    Path to remove nodes from.\n    - **Default:** Entire document\n  </APIItem>\n</APIOptions>\n</API>\n\n### `tf.ai.undo()`\n\nSpecial undo operation for AI changes:\n- Undoes the last operation if it was AI-generated\n- Removes the redo stack entry to prevent redoing AI operations\n\n## Streaming Behavior\n\n### Enhanced Empty Paragraph Handling\n\nThe AI streaming system intelligently handles empty paragraphs:\n- Only removes truly empty paragraphs when starting to stream\n- Preserves paragraphs containing only whitespace or formatting marks\n- Prevents accidental content loss during streaming initialization\n\n### Table and Column Support\n\nAI streaming seamlessly works within complex structures:\n\n**Tables:**\n- Streams directly into table cells without disrupting table structure\n- Maintains table formatting during streaming\n- Properly handles cell boundaries\n\n**Columns:**\n- Supports streaming into column layouts\n- Preserves column width and structure\n- Enables AI content generation within multi-column documents\n\n### MDX Tag Preservation\n\nDuring streaming, the system:\n- Detects and preserves custom MDX tags\n- Prevents MDX content from being incorrectly parsed as Markdown\n- Maintains proper nesting of MDX elements\n- Supports streaming of content containing MDX components\n\n## Hooks\n\n### `useAIChatEditor`\n\nA hook that registers an editor in the AI chat plugin and deserializes markdown content with block-level memoization.\n\n<API name=\"useAIChatEditor\">\n<APIParameters>\n  <APIItem name=\"editor\" type=\"PlateEditor\">\n    The editor instance to register.\n  </APIItem>\n  <APIItem name=\"content\" type=\"string\">\n    The markdown content to deserialize into the editor.\n  </APIItem>\n  <APIItem name=\"options\" type=\"object\" optional>\n    Options for content processing.\n  </APIItem>\n</APIParameters>\n\n<APIOptions type=\"object\">\n  <APIItem name=\"memoize\" type=\"boolean\" optional>\n    Enable block-level memoization with `_memo` property.\n    - **Default:** `true`\n  </APIItem>\n  <APIItem name=\"parser\" type=\"ParseMarkdownBlocksOptions\" optional>\n    Options for the markdown token parser. Can filter out specific token types.\n  </APIItem>\n  <APIItem name=\"processor\" type=\"(processor: Processor) => Processor\" optional>\n    Function to customize the markdown processor.\n  </APIItem>\n</APIOptions>\n\n```tsx\nconst AIChatEditor = ({ content }: { content: string }) => {\n  const aiEditor = usePlateEditor({\n    plugins: [\n      // Your editor plugins\n      MarkdownPlugin,\n      AIPlugin,\n      AIChatPlugin,\n      // etc...\n    ],\n  });\n\n  useAIChatEditor(aiEditor, content, {\n    // Optional markdown parser options\n    parser: {\n      exclude: ['space'],\n    },\n  });\n\n  return <Editor editor={aiEditor} />;\n};\n```\n</API>\n",
      "type": "registry:file",
      "target": "content/docs/plate/(plugins)/(ai)/ai.mdx"
    }
  ]
}